#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import requests
import gzip
import shutil
import subprocess
import json
import logging # ğŸ¾ æ–°å¢ï¼šå¯¼å…¥æ—¥å¿—æ¨¡å—
import hashlib # ğŸ¾ æ–°å¢ï¼šå¯¼å…¥å“ˆå¸Œè®¡ç®—æ¨¡å—
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from zoneinfo import ZoneInfo

# ================ æ—¥å¿—é…ç½® ================
# ğŸ¾ æ–°å¢ï¼šé…ç½®æ—¥å¿—è®°å½•å™¨
def setup_logging(log_file_path: Path):
    """é…ç½®æ—¥å¿—ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    # log_file_path.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            # logging.FileHandler(log_file_path, encoding='utf-8'),
            logging.StreamHandler() # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )

# ================ é…ç½®éƒ¨åˆ† ================
BASE_DIR = Path(__file__).parent
WORK_DIR = BASE_DIR / "tmp"
REPO_DIR = BASE_DIR / "nothing"
OUTPUT_DIR = REPO_DIR / "mrs"
LOG_FILE = WORK_DIR / "script_run.log" # ğŸ¾ æ–°å¢ï¼šå®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„

# --- æ•°æ®æºé…ç½® (ä¿æŒä¸å˜) ---
AD_SOURCES = [
    {"url": "https://raw.githubusercontent.com/privacy-protection-tools/anti-AD/master/anti-ad-clash.yaml", "format_override": "yaml", "processors": ["remove_comments_and_empty"]},
    {"url": "https://github.com/Cats-Team/AdRules/main/adrules_domainset.txt", "format_override": "text", "processors": ["remove_comments_and_empty", "format_yaml_list"]},
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/refs/heads/meta/geo/geosite/category-httpdns-cn@ads.list", "format_override": "text", "processors": ["remove_comments_and_empty", "format_yaml_list"]},
    {"url": "https://github.com/ignaciocastro/a-dove-is-dumb/raw/refs/heads/main/pihole.txt", "format_override": "text", "processors": ["remove_comments_and_empty", "format_pihole"]},
]
CN_SOURCES = [
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geosite/cn.list"},
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geosite/steam@cn.list"},
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geosite/microsoft@cn.list"},
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geosite/google@cn.list"},
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geosite/win-update.list"},
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geosite/private.list"},
]
CNIP_SOURCES = [
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geoip/cn.list"},
    {"url": "https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geoip/private.list"},
]

# ================ æ–‡æœ¬å¤„ç†å‡½æ•° (ä¿æŒä¸å˜) ================
def remove_comments_and_empty(text: str) -> str:
    lines = [line for line in text.splitlines() if line.strip() and not line.strip().startswith('#')]
    return "\n".join(lines)
def add_prefix_suffix(text: str, prefix: str, suffix: str) -> str:
    lines = [f"{prefix}{line}{suffix}" for line in text.splitlines()]
    return "\n".join(lines)
def format_pihole(text: str) -> str:
    return add_prefix_suffix(text, "  - '+.", "'")
def format_yaml_list(text: str) -> str:
    return add_prefix_suffix(text, "  - '", "'")
PROCESSORS = {
    "remove_comments_and_empty": remove_comments_and_empty,
    "format_pihole": format_pihole,
    "format_yaml_list": format_yaml_list,
}

# ================ å·¥å…·å‡½æ•° ================
# ğŸ¾ æ–°å¢ï¼šè®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼çš„è¾…åŠ©å‡½æ•°
def get_file_sha256(file_path: Path) -> str | None:
    """è®¡ç®—æ–‡ä»¶çš„SHA256å“ˆå¸Œå€¼ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None"""
    if not file_path.exists():
        return None
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        # Read and update hash string value in blocks of 4K
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def init_env():
    """åˆå§‹åŒ–ç¯å¢ƒ"""
    logging.info("æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...")
    WORK_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    mihomo_path = WORK_DIR / "mihomo"
    if mihomo_path.exists():
        logging.info("Mihomo å·¥å…·å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
        return

    try:
        api_url = "https://api.github.com/repos/MetaCubeX/mihomo/releases"
        logging.info(f"æ­£åœ¨ä»GitHub APIè·å–Mihomoå‘å¸ƒä¿¡æ¯: {api_url}")
        response = requests.get(api_url, timeout=30)
        response.raise_for_status()
        releases = response.json()
        
        # ... (ä¸‹è½½é€»è¾‘ä¿æŒä¸å˜ï¼Œä½†æ‰“å°æ”¹ä¸ºäº†æ—¥å¿—è®°å½•)
        download_url = ""
        for release in releases:
            if "Prerelease-Alpha" in release.get("tag_name", ""):
                for asset in release.get("assets", []):
                    if "mihomo-linux-amd64-alpha" in asset.get("name", "") and asset.get("name", "").endswith(".gz"):
                        download_url = asset.get("browser_download_url")
                        break
            if download_url:
                break
        
        if not download_url:
            logging.error("æ— æ³•è·å–Mihomoä¸‹è½½é“¾æ¥")
            exit(1)

        logging.info(f"ä» {download_url} ä¸‹è½½ä¸­...")
        gz_path = WORK_DIR / "mihomo.gz"
        with requests.get(download_url, stream=True, timeout=60) as r:
            r.raise_for_status()
            with open(gz_path, 'wb') as f: shutil.copyfileobj(r.raw, f) # type: ignore
        
        logging.info("è§£å‹ Mihomo...")
        with gzip.open(gz_path, 'rb') as f_in:
            with open(mihomo_path, 'wb') as f_out: shutil.copyfileobj(f_in, f_out)
        
        gz_path.unlink()
        mihomo_path.chmod(0o755)
        logging.info("ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

    except Exception as e:
        logging.critical(f"ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        exit(1)

def download_and_process_one(source_info: dict, index: int) -> tuple[int, str]:
    """ä¸‹è½½å¹¶å¤„ç†å•ä¸ªæº"""
    url = source_info["url"]
    logging.info(f"å¼€å§‹ä¸‹è½½: {url}")
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        content = response.text
        
        processor_names = source_info.get("processors")
        if processor_names:
            logging.info(f"å¯¹ {url} åº”ç”¨è‡ªå®šä¹‰å¤„ç†: {', '.join(processor_names)}")
            for name in processor_names: content = PROCESSORS[name](content)
        else:
            content = remove_comments_and_empty(content)

        if not content.endswith('\n'): content += '\n'
        
        file_size = len(content.encode('utf-8'))
        logging.info(f"ä¸‹è½½å¹¶å¤„ç†å®Œæˆ: {url} (å¤§å°: {file_size} å­—èŠ‚)")
        if file_size == 0:
            logging.warning(f"ä» {url} è·å–çš„å†…å®¹ä¸ºç©º")
        
        return index, content
    except requests.RequestException as e:
        logging.warning(f"ä¸‹è½½æˆ–å¤„ç†å¤±è´¥: {url} - é”™è¯¯: {e}")
        return index, ""

def process_ruleset_group(name: str, rule_type: str, final_format: str, sources: list):
    """å¹¶è¡Œå¤„ç†ä¸€ç»„è§„åˆ™ï¼Œå¹¶éªŒè¯æ–‡ä»¶æ›´æ–°"""
    logging.info(f"å¼€å§‹å¤„ç† {name} è§„åˆ™é›†...")
    
    results = {}
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_source = {executor.submit(download_and_process_one, source, i): i for i, source in enumerate(sources)}
        for future in as_completed(future_to_source):
            index, content = future.result()
            results[index] = content

    logging.info(f"åˆå¹¶ {name} çš„æ‰€æœ‰æº...")
    ordered_contents = [results[i] for i in sorted(results.keys())]
    combined_content = "".join(ordered_contents)

    if not combined_content.strip():
        logging.error(f"{name} è§„åˆ™é›†å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡æ­¤è§„åˆ™é›†çš„å¤„ç†ã€‚")
        return
        
    mihomo_executable = WORK_DIR / "mihomo"
    # ğŸ¾ ä¿®æ”¹ï¼šä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ¥å­˜æ”¾æœ€ç»ˆçš„ .mrs æ–‡ä»¶ï¼Œä»¥ä¾¿æ¯”è¾ƒ
    temp_mrs_path = WORK_DIR / f"{name}.mrs.tmp"
    final_mrs_path = OUTPUT_DIR / f"{name}.mrs"

    # --- æ–‡æœ¬é¢„å¤„ç†é€»è¾‘ (å°† print æ”¹ä¸º logging.debug) ---
    temp_processed_path = WORK_DIR / f"{name}.{'yaml' if final_format == 'yaml' else 'text'}"
    if final_format == "yaml":
        lines = combined_content.splitlines()
        header = lines[0]
        body_lines = sorted(list(set(filter(None, lines[1:]))))
        final_list_content = header + "\n" + "\n".join(body_lines)
        temp_processed_path.write_text(final_list_content, encoding='utf-8')
    else:
        lines = filter(None, combined_content.splitlines())
        unique_sorted_lines = sorted(list(set(lines)))
        final_text_content = "\n".join(unique_sorted_lines)
        temp_processed_path.write_text(final_text_content, encoding='utf-8')

    logging.info(f"åˆå¹¶åçš„æ–‡ä»¶ '{temp_processed_path.name}' å¤§å°: {temp_processed_path.stat().st_size} å­—èŠ‚ï¼Œå¼€å§‹è½¬æ¢...")
    
    # --- è°ƒç”¨ mihomo è¿›è¡Œè½¬æ¢ ---
    try:
        subprocess.run(
            [str(mihomo_executable), "convert-ruleset", rule_type, final_format, str(temp_processed_path), str(temp_mrs_path)],
            check=True, capture_output=True, text=True
        )
    except subprocess.CalledProcessError as e:
        logging.error(f"Mihomo è½¬æ¢å¤±è´¥ for {name}: {e.stderr}")
        return

    # --- ğŸ¾ æ ¸å¿ƒæ”¹è¿›ï¼šæ–‡ä»¶æ›´æ–°éªŒè¯ ---
    old_hash = get_file_sha256(final_mrs_path)
    new_hash = get_file_sha256(temp_mrs_path)

    if old_hash == new_hash:
        logging.info(f"âœ”ï¸ æ–‡ä»¶ '{final_mrs_path.name}' å†…å®¹æœªå‘ç”Ÿå˜åŒ–ï¼Œæ— éœ€æ›¿æ¢ã€‚ (SHA256: {new_hash[:12]}...)") # type: ignore
        temp_mrs_path.unlink() # åˆ é™¤ä¸´æ—¶ç”Ÿæˆçš„æ–°æ–‡ä»¶
    else:
        logging.info(f"âœ¨ æ–‡ä»¶ '{final_mrs_path.name}' å·²æ›´æ–°ï¼æ­£åœ¨æ›¿æ¢...")
        logging.info(f"   æ—§å“ˆå¸Œ: {old_hash[:12] if old_hash else 'N/A'}...")
        logging.info(f"   æ–°å“ˆå¸Œ: {new_hash[:12]}...") # type: ignore
        shutil.move(temp_mrs_path, final_mrs_path) # ç”¨æ–°æ–‡ä»¶è¦†ç›–æ—§æ–‡ä»¶

    # æ¸…ç†ä¸­é—´æ–‡ä»¶
    temp_processed_path.unlink()
    logging.info(f"âœ… {name} è§„åˆ™é›†å¤„ç†å®Œæˆã€‚")

def commit_changes():
    """æäº¤æ›´æ”¹åˆ°Gitä»“åº“"""
    logging.info("å‡†å¤‡æäº¤æ›´æ”¹åˆ°Gitä»“åº“...")
    
    if not shutil.which("git"):
        logging.error("æœªæ‰¾åˆ° 'git' å‘½ä»¤ï¼Œæ— æ³•æäº¤ã€‚")
        return

    try:
        shanghai_tz = ZoneInfo("Asia/Shanghai")
        commit_time = datetime.now(shanghai_tz).strftime('%Y-%m-%d %H:%M:%S')
        commit_message = f"{commit_time} æ›´æ–°mrsè§„åˆ™"

        subprocess.run(["git", "-C", str(REPO_DIR), "config", "--local", "user.email", "actions@github.com"], check=True)
        subprocess.run(["git", "-C", str(REPO_DIR), "config", "--local", "user.name", "GitHub Actions"], check=True)
        logging.info("æ­£åœ¨æ‹‰å–è¿œç¨‹ä»“åº“æ›´æ–°...")
        subprocess.run(["git", "-C", str(REPO_DIR), "pull", "origin", "main"], check=True)
        logging.info("æ·»åŠ æ–°ç”Ÿæˆçš„æ–‡ä»¶åˆ°æš‚å­˜åŒº...")
        subprocess.run(["git", "-C", str(REPO_DIR), "add", "mrs/*"], check=True)
        
        result = subprocess.run(["git", "-C", str(REPO_DIR), "commit", "-m", commit_message], capture_output=True, text=True)
        if result.returncode == 0:
            logging.info("Git commitæˆåŠŸï¼")
            logging.info(result.stdout)
        elif "nothing to commit" in result.stdout or "æ— æ–‡ä»¶è¦æäº¤" in result.stdout:
            logging.info("âœ”ï¸ Gitä»“åº“ä¸­æ²¡æœ‰éœ€è¦æäº¤çš„æ›´æ”¹ã€‚")
        else:
            logging.error(f"Git commit å¤±è´¥: {result.stderr}")

    except FileNotFoundError:
        logging.error(f"Gitä»“åº“ç›®å½• '{REPO_DIR}' ä¸å­˜åœ¨ã€‚")
    except subprocess.CalledProcessError as e:
        logging.error(f"æ‰§è¡ŒGitå‘½ä»¤æ—¶å‡ºé”™: {e.stderr}")
    except Exception as e:
        logging.error(f"æäº¤æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# ================ ä¸»æ‰§è¡Œæµç¨‹ ================
def main():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£è°ƒåº¦æ‰€æœ‰ä»»åŠ¡"""
    setup_logging(LOG_FILE) # ğŸ¾ æ–°å¢ï¼šåœ¨å¼€å§‹æ—¶è®¾ç½®å¥½æ—¥å¿—
    logging.info("================== è„šæœ¬å¼€å§‹è¿è¡Œ ==================")
    
    try:
        init_env()
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(process_ruleset_group, "ad", "domain", "yaml", AD_SOURCES),
                executor.submit(process_ruleset_group, "cn", "domain", "text", CN_SOURCES),
                executor.submit(process_ruleset_group, "cnIP", "ipcidr", "text", CNIP_SOURCES)
            ]
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logging.error(f"ä¸€ä¸ªè§„åˆ™é›†å¤„ç†ä»»åŠ¡æ‰§è¡Œæ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        
        commit_changes()
    
    except Exception as e:
        logging.critical(f"è„šæœ¬åœ¨ä¸»æµç¨‹ä¸­é­é‡è‡´å‘½é”™è¯¯ï¼Œå·²ç»ˆæ­¢: {e}", exc_info=True)
    finally:
        logging.info("================== è„šæœ¬è¿è¡Œç»“æŸ ==================\n")


if __name__ == "__main__":
    main()